{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import library\n",
    "import numpy\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = numpy.random.rand(100000000)\n",
    "y = numpy.random.rand(100000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to perform dot product between x and y. dot product refered by x.y.\n",
    "x = [x1,x2,x3]\n",
    "y = [y1,y2,y3]\n",
    "dot product can be calculated as\n",
    "x.y = x1*y1 + x2*y2 + x3*y3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now first check non vectorized scaler version of the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken by scaler program : 68.59447741508484\n",
      "Sum value : 24999772.4968\n"
     ]
    }
   ],
   "source": [
    "time1 = time.time()\n",
    "sum = 0\n",
    "for i in range(len(y)):\n",
    "    sum = sum+x[i]*y[i]\n",
    "time2 = time.time()\n",
    "print('Time taken by scaler program :',str(time2-time1))\n",
    "print('Sum value :', str(sum))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we check vectorized version available in numpy library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken by scaler program : 0.4517829418182373\n",
      "Sum value : 24999772.4968\n"
     ]
    }
   ],
   "source": [
    "time1 = time.time()\n",
    "sum = 0\n",
    "sum = numpy.dot(x,y)\n",
    "time2 = time.time()\n",
    "print('Time taken by scaler program :',str(time2-time1))\n",
    "print('Sum value :', str(sum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Creating ndarray\n",
    "#Create one dimensional  array\n",
    "arrlist = [1,2,3,4]\n",
    "arrNumpy = numpy.array(arrlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Multi dimension array\n",
    "arrlist2 = [[1,2,3,4],[5,6,7,8]]\n",
    "arrNumpy2 = numpy.array(arrlist2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "(2, 4)\n"
     ]
    }
   ],
   "source": [
    "#Print dimention of array\n",
    "print(arrNumpy2.ndim)\n",
    "\n",
    "#Print shape of array\n",
    "print(arrNumpy2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero Array  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "Ones Array [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "Empty Array [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "#Create array of zeros \n",
    "zero_array = numpy.zeros(10)\n",
    "print('Zero Array ',zero_array)\n",
    "\n",
    "#Create array of ones\n",
    "ones_array = numpy.ones(10)\n",
    "print('Ones Array', ones_array)\n",
    "\n",
    "#Create empty array\n",
    "empty_array = numpy.empty(10)\n",
    "print('Empty Array', empty_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3. ,  3.2,  3.4,  3.6,  3.8,  4. ,  4.2,  4.4,  4.6,  4.8,  5. ,\n",
       "        5.2,  5.4,  5.6,  5.8])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create array of numbers\n",
    "#Generate numbers from 0 upto length 10\n",
    "numpy.arange(10)\n",
    "\n",
    "#Generate numbers between a range\n",
    "numpy.arange(-5,14)\n",
    "\n",
    "#Generate numbers with a fixed distance between them\n",
    "numpy.arange(3,6,0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indexing and Slicing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
      "2\n",
      "[2 3 4]\n",
      "[ 0  1  2 34 34 34 34 34 34  9 10 11 12 13 14]\n"
     ]
    }
   ],
   "source": [
    "#Indexing and slicing on 1D array\n",
    "array = numpy.arange(15)\n",
    "print(array)\n",
    "print(array[2])\n",
    "print(array[2:5])\n",
    "array[3:9] = 34\n",
    "print(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [7 8 9]]\n",
      "[7 8 9]\n",
      "8\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "array2 = numpy.array([[1,2,3],[7,8,9]])\n",
    "print(array2)\n",
    "print(array2[1])\n",
    "print(array2[1][1])\n",
    "print(array2[1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 4 5 6 7 8 9]\n",
      "[[ 7  8  9]\n",
      " [13 14 15]]\n",
      "[[ 8  9]\n",
      " [14 15]]\n"
     ]
    }
   ],
   "source": [
    "#More example on slicing\n",
    "array = numpy.arange(10)\n",
    "array2 = array2 = numpy.array([[1,2,3],[7,8,9],[13,14,15]])\n",
    "print(array[3:])\n",
    "print(array2[1:])\n",
    "print(array2[1:,1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boolean Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False  True False False False]\n",
      "['Ankita' 'Ankita']\n",
      "[11 12 13 14]\n"
     ]
    }
   ],
   "source": [
    "names = ['Abhishek','Ankita','Vijay','Aditya','John','Ankita']\n",
    "names_array = numpy.array(names)\n",
    "\n",
    "#Check where the condition is true\n",
    "print(names_array=='Vijay')\n",
    "\n",
    "#Include names which satisfy the condition\n",
    "namelist = names_array[names_array=='Ankita']\n",
    "print(namelist)\n",
    "\n",
    "array = numpy.arange(15)\n",
    "large10  = array[array>10]\n",
    "print(large10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fast Element wise functions on array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Software\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: RuntimeWarning: divide by zero encountered in log\n",
      "  app.launch_new_instance()\n",
      "C:\\Software\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:17: RuntimeWarning: divide by zero encountered in log10\n",
      "C:\\Software\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:27: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.,  1.,  1.,  1.,  1.,  1.])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array = numpy.arange(-5,5)\n",
    "\n",
    "#Absolute values of element\n",
    "numpy.abs(array)\n",
    "\n",
    "#Square root of each element\n",
    "numpy.sqrt(numpy.abs(array))\n",
    "\n",
    "#Square\n",
    "numpy.square(array)\n",
    "\n",
    "#Exponent\n",
    "numpy.exp(array)\n",
    "\n",
    "#Logrithm\n",
    "numpy.log(numpy.abs(array))\n",
    "numpy.log10(numpy.abs(array))\n",
    "\n",
    "#Sign\n",
    "numpy.sign(array)\n",
    "\n",
    "#Ceil and Floor\n",
    "numpy.ceil(array)\n",
    "numpy.floor(array)\n",
    "\n",
    "#Checking for infinite\n",
    "numpy.isinf(numpy.log(numpy.abs(array)))\n",
    "\n",
    "arrayTrig = numpy.array([0,45,90,180,270,370])\n",
    "\n",
    "#trigonometric  functions\n",
    "numpy.sin(arrayTrig)\n",
    "numpy.cos(arrayTrig)\n",
    "numpy.tan(arrayTrig)\n",
    "numpy.sinh(arrayTrig)\n",
    "numpy.cosh(arrayTrig)\n",
    "numpy.tanh(arrayTrig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  4,  7, 10, 13, 16, 19, 22, 25, 28, 31, 34, 37, 40, 43, 46, 49,\n",
       "       52, 55, 58])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array1 = numpy.arange(1,60,3)\n",
    "array2 = numpy.arange(1,80,4)\n",
    "\n",
    "#Add two array (element wise)\n",
    "numpy.add(array1,array2)\n",
    "\n",
    "#Substract two array\n",
    "numpy.subtract(array2,array1)\n",
    "\n",
    "#Multiply two array\n",
    "numpy.multiply(array1,array2)\n",
    "\n",
    "#Divide\n",
    "numpy.divide(array2,array1)\n",
    "\n",
    "#Power\n",
    "numpy.power(array2,array1)\n",
    "\n",
    "#return element wise maximum\n",
    "numpy.maximum(array2,array1)\n",
    "\n",
    "#return element wise minimum\n",
    "numpy.minimum(array2,array1)\n",
    "\n",
    "#Logical operations\n",
    "numpy.logical_and(array1,array2)\n",
    "numpy.logical_not(array1)\n",
    "numpy.logical_or(array1,array2)\n",
    "numpy.logical_xor(array1,array2)\n",
    "\n",
    "#Sort array\n",
    "numpy.sort(array1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistical Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1,   5,  12,  22,  35,  51,  70,  92, 117, 145, 176, 210, 247,\n",
       "       287, 330, 376, 425, 477, 532, 590], dtype=int32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sum of all elements\n",
    "numpy.sum(array1)\n",
    "\n",
    "#Mean\n",
    "numpy.mean(array1)\n",
    "\n",
    "#Standard Deviation\n",
    "numpy.std(array1)\n",
    "\n",
    "#Variance\n",
    "numpy.var(array1)\n",
    "\n",
    "#Maximum element\n",
    "numpy.max(array1)\n",
    "\n",
    "#Minimum element\n",
    "numpy.min(array1)\n",
    "\n",
    "#Index of max element\n",
    "numpy.argmax(array1)\n",
    "\n",
    "#Index of max element\n",
    "numpy.argmin(array1)\n",
    "\n",
    "#Cumulative product of elements\n",
    "numpy.cumprod(array1)\n",
    "\n",
    "#Cumulative sum of elements\n",
    "numpy.cumsum(array1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4,  7, 10, 16, 19, 22, 28, 31, 34, 40, 43, 46, 52, 55, 58])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Union of two array\n",
    "numpy.union1d(array1,array2)\n",
    "\n",
    "#Intersection of two array\n",
    "numpy.intersect1d(array1,array2)\n",
    "\n",
    "#Set different\n",
    "numpy.setdiff1d(array1,array2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Algebra Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Dot product sum\n",
    "array1.dot(array2)\n",
    "\n",
    "array2d1 = numpy.array([[1,2,3],[4,5,6],[7,8,9]])\n",
    "array2d2 = numpy.array([[1,1,1],[2,2,2],[3,3,3]])\n",
    "\n",
    "#Inverse of matrix\n",
    "numpy.linalg.inv(array2d1)\n",
    "\n",
    "#Transposne\n",
    "array2d1.T\n",
    "\n",
    "#Get diagonal of matrix\n",
    "numpy.diag(array2d1)\n",
    "\n",
    "#Compute determinant\n",
    "numpy.linalg.det(array2d1)\n",
    "\n",
    "#QR Decomposition of matrix\n",
    "q,r = numpy.linalg.qr(array2d1)\n",
    "\n",
    "#Compute trace (sum of all diagonal element)\n",
    "numpy.trace(array2d1)\n",
    "\n",
    "#SVD decomposition\n",
    "a,b,c = numpy.linalg.svd(array2d1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid_array(x):                                        \n",
    "    return 1 / (1 + numpy.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numbers = numpy.arange(-10,10)\n",
    "sigmoids = sigmoid_array(numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nof8PKMyvKVQcVokAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x26287dbd160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(numbers, sigmoids)\n",
    "ax.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tanh_array(x):                                        \n",
    "    return numpy.tanh(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numbers = numpy.arange(-10,10)\n",
    "tanh = tanh_array(numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAD8CAYAAABzTgP2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x262e8286828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(numbers, tanh)\n",
    "ax.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu_array(x):                                        \n",
    "    return numpy.maximum(0,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nsAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x262e8348fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### relu = relu_array(numbers)\n",
    "relu = relu_array(numbers)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(numbers, relu)\n",
    "ax.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Neural Network Classifier using Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build our first simple neural network classifier using keras library. For this experiment we will use iris dataset. This is a public dataset downloaded from UCI https://archive.ics.uci.edu/ml/datasets/iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this task we need to install two additional libraries Keras and Tensorflow.\n",
    "\n",
    "use following command to install keras and tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in c:\\software\\anaconda3\\lib\\site-packages\n",
      "Requirement already satisfied: pyyaml in c:\\software\\anaconda3\\lib\\site-packages (from keras)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\software\\anaconda3\\lib\\site-packages (from keras)\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\software\\anaconda3\\lib\\site-packages (from keras)\n",
      "Requirement already satisfied: scipy>=0.14 in c:\\software\\anaconda3\\lib\\site-packages (from keras)\n",
      "Requirement already satisfied: tensorflow in c:\\software\\anaconda3\\lib\\site-packages\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\software\\anaconda3\\lib\\site-packages (from tensorflow)\n",
      "Requirement already satisfied: protobuf>=3.3.0 in c:\\software\\anaconda3\\lib\\site-packages (from tensorflow)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\software\\anaconda3\\lib\\site-packages (from tensorflow)\n",
      "Requirement already satisfied: tensorflow-tensorboard<0.2.0,>=0.1.0 in c:\\software\\anaconda3\\lib\\site-packages (from tensorflow)\n",
      "Requirement already satisfied: numpy>=1.11.0 in c:\\software\\anaconda3\\lib\\site-packages (from tensorflow)\n",
      "Requirement already satisfied: setuptools in c:\\software\\anaconda3\\lib\\site-packages\\setuptools-27.2.0-py3.6.egg (from protobuf>=3.3.0->tensorflow)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\software\\anaconda3\\lib\\site-packages (from tensorflow-tensorboard<0.2.0,>=0.1.0->tensorflow)\n",
      "Requirement already satisfied: html5lib==0.9999999 in c:\\software\\anaconda3\\lib\\site-packages (from tensorflow-tensorboard<0.2.0,>=0.1.0->tensorflow)\n",
      "Requirement already satisfied: bleach==1.5.0 in c:\\software\\anaconda3\\lib\\site-packages (from tensorflow-tensorboard<0.2.0,>=0.1.0->tensorflow)\n",
      "Requirement already satisfied: werkzeug>=0.11.10 in c:\\software\\anaconda3\\lib\\site-packages (from tensorflow-tensorboard<0.2.0,>=0.1.0->tensorflow)\n"
     ]
    }
   ],
   "source": [
    "! pip install keras\n",
    "! pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Software\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Import libraries\n",
    "import seaborn\n",
    "import numpy\n",
    "import pandas \n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Read iris dataset stored in data folder\n",
    "data = pandas.read_csv('D:/book/data/iris.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width        class\n",
       "0           5.1          3.5           1.4          0.2  Iris-setosa\n",
       "1           4.9          3.0           1.4          0.2  Iris-setosa\n",
       "2           4.7          3.2           1.3          0.2  Iris-setosa\n",
       "3           4.6          3.1           1.5          0.2  Iris-setosa\n",
       "4           5.0          3.6           1.4          0.2  Iris-setosa"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check initial five rows of the data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above data has three classes in text form. To give input to our neural network model, we first need to convert them to one hot encoding. We use pandas get_dummies() function to create one hot encoded class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class_as_one_hot_encoding = pandas.get_dummies(data['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Remove class and concat one hot encoding values\n",
    "del data['class']\n",
    "data = pandas.concat([data, class_as_one_hot_encoding], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Separate independent and dependent attributes\n",
    "x = data.values[:,:4]\n",
    "y = data.values[:,4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Training and test split\n",
    "train_x, test_x, train_y, test_y = train_test_split(x, y, train_size=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras is a library of neural network. It has a lot of hyper parameters to tune for better model creation. In the dataset which we have choose contains 4 features and one output with three possible classes. So our input layer must have four units and output layer must have three units. We have a choice of number of hidden layers and hidden units within. \n",
    "\n",
    "In this example we will use only one hidden layer. We will take 10 hidden units in it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use sequential model in keras. This model is used to stacking up layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define our first hidden layer. This layer is connected with four attribute input layer. In Keras we have different type of layers\n",
    "\n",
    "1. Dense Layer  - Nodes are fully connected with the output layer nodes.\n",
    "2. Activation Layer - Contains activation functions like sigmoid, ReLu, tanh\n",
    "3. Dropout Layer- Used for regularization\n",
    "4. Flatten Layer - Flatten the input.\n",
    "5. Reshape Layer - Reshape the output.\n",
    "6. Permute Layer - Permute the dimension of input as per pattern\n",
    "7. Repeat vector - repeat the input n times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our example we will use Dense layer followed by activation layer. In our first simple neural network we want to connect each input layer node to output layer node. We will use 10 hidden units in hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Dense(10, input_shape=(4,)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use sigmoid activation for this layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define output layer. As we have 3 output classes, we use three units in output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Dense(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use softmax activation function for output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Configure Learning Process**\n",
    "\n",
    "Now we configure neural network training process. We have following choices.\n",
    "\n",
    "1. Optimizer\n",
    "    * Stochastic Gradient Descent\n",
    "    * Adamax\n",
    "    * Adam\n",
    "    * Nadam\n",
    "    * TFOptimizer\n",
    "    * RMSprop\n",
    "\n",
    "2. Loss function : Loss function is used to create the model for prediction. Different type of matrices are defined for different type of data. Some of the popular loss functions are\n",
    "\n",
    "    * mean_squared_error\n",
    "    * mean_absolute_error\n",
    "    * binary-cross-entropy\n",
    "    * categorical_crossentropy\n",
    "    * poisson\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we compile our model using compile() function. Compiling the model uses efficient library (Tensorflow in this case) to do the processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we fit the model. We can choose different set of hyper parameters which affects our training. Here we have chosen number of iteration = 100 (epochs) and batch size =1. Batch size = 1 means at one time we are exposing only one training example to our model. If we don't want to see the processing and loss calculation, we can add parameter verbose=0 in the fit function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "105/105 [==============================] - 0s - loss: 1.1276 - acc: 0.4286         \n",
      "Epoch 2/100\n",
      "105/105 [==============================] - 0s - loss: 1.0631 - acc: 0.3524        \n",
      "Epoch 3/100\n",
      "105/105 [==============================] - 0s - loss: 1.0190 - acc: 0.4381        \n",
      "Epoch 4/100\n",
      "105/105 [==============================] - 0s - loss: 0.9865 - acc: 0.5238     \n",
      "Epoch 5/100\n",
      "105/105 [==============================] - 0s - loss: 0.9491 - acc: 0.4571     \n",
      "Epoch 6/100\n",
      "105/105 [==============================] - 0s - loss: 0.9135 - acc: 0.5905        \n",
      "Epoch 7/100\n",
      "105/105 [==============================] - 0s - loss: 0.8679 - acc: 0.7714     \n",
      "Epoch 8/100\n",
      "105/105 [==============================] - 0s - loss: 0.8219 - acc: 0.9333     \n",
      "Epoch 9/100\n",
      "105/105 [==============================] - 0s - loss: 0.7761 - acc: 0.9143     \n",
      "Epoch 10/100\n",
      "105/105 [==============================] - 0s - loss: 0.7360 - acc: 0.8857     \n",
      "Epoch 11/100\n",
      "105/105 [==============================] - 0s - loss: 0.6995 - acc: 0.9143     \n",
      "Epoch 12/100\n",
      "105/105 [==============================] - 0s - loss: 0.6683 - acc: 0.9048     \n",
      "Epoch 13/100\n",
      "105/105 [==============================] - 0s - loss: 0.6412 - acc: 0.8762     \n",
      "Epoch 14/100\n",
      "105/105 [==============================] - 0s - loss: 0.6151 - acc: 0.9429     \n",
      "Epoch 15/100\n",
      "105/105 [==============================] - 0s - loss: 0.5901 - acc: 0.9238     \n",
      "Epoch 16/100\n",
      "105/105 [==============================] - 0s - loss: 0.5721 - acc: 0.8857     \n",
      "Epoch 17/100\n",
      "105/105 [==============================] - 0s - loss: 0.5545 - acc: 0.9238     \n",
      "Epoch 18/100\n",
      "105/105 [==============================] - 0s - loss: 0.5366 - acc: 0.9714     \n",
      "Epoch 19/100\n",
      "105/105 [==============================] - 0s - loss: 0.5203 - acc: 0.9619     \n",
      "Epoch 20/100\n",
      "105/105 [==============================] - 0s - loss: 0.5068 - acc: 0.9524     \n",
      "Epoch 21/100\n",
      "105/105 [==============================] - 0s - loss: 0.4934 - acc: 0.9429     \n",
      "Epoch 22/100\n",
      "105/105 [==============================] - 0s - loss: 0.4832 - acc: 0.9619     \n",
      "Epoch 23/100\n",
      "105/105 [==============================] - 0s - loss: 0.4722 - acc: 0.9619     \n",
      "Epoch 24/100\n",
      "105/105 [==============================] - 0s - loss: 0.4633 - acc: 0.9619     \n",
      "Epoch 25/100\n",
      "105/105 [==============================] - 0s - loss: 0.4517 - acc: 0.9429     \n",
      "Epoch 26/100\n",
      "105/105 [==============================] - 0s - loss: 0.4423 - acc: 0.9714     \n",
      "Epoch 27/100\n",
      "105/105 [==============================] - 0s - loss: 0.4325 - acc: 0.9238     \n",
      "Epoch 28/100\n",
      "105/105 [==============================] - 0s - loss: 0.4260 - acc: 0.9619     \n",
      "Epoch 29/100\n",
      "105/105 [==============================] - 0s - loss: 0.4173 - acc: 0.9619     \n",
      "Epoch 30/100\n",
      "105/105 [==============================] - 0s - loss: 0.4123 - acc: 0.9619     \n",
      "Epoch 31/100\n",
      "105/105 [==============================] - 0s - loss: 0.4048 - acc: 0.9714     \n",
      "Epoch 32/100\n",
      "105/105 [==============================] - 0s - loss: 0.3953 - acc: 0.9714     \n",
      "Epoch 33/100\n",
      "105/105 [==============================] - 0s - loss: 0.3895 - acc: 0.9714     \n",
      "Epoch 34/100\n",
      "105/105 [==============================] - 0s - loss: 0.3813 - acc: 0.9714     \n",
      "Epoch 35/100\n",
      "105/105 [==============================] - 0s - loss: 0.3742 - acc: 0.9810     \n",
      "Epoch 36/100\n",
      "105/105 [==============================] - 0s - loss: 0.3690 - acc: 0.9714     \n",
      "Epoch 37/100\n",
      "105/105 [==============================] - 0s - loss: 0.3616 - acc: 0.9714     \n",
      "Epoch 38/100\n",
      "105/105 [==============================] - 0s - loss: 0.3534 - acc: 0.9810       \n",
      "Epoch 39/100\n",
      "105/105 [==============================] - 0s - loss: 0.3524 - acc: 0.9714     \n",
      "Epoch 40/100\n",
      "105/105 [==============================] - 0s - loss: 0.3444 - acc: 0.9810     \n",
      "Epoch 41/100\n",
      "105/105 [==============================] - 0s - loss: 0.3378 - acc: 0.9810     \n",
      "Epoch 42/100\n",
      "105/105 [==============================] - 0s - loss: 0.3322 - acc: 0.9619     \n",
      "Epoch 43/100\n",
      "105/105 [==============================] - 0s - loss: 0.3249 - acc: 0.9714     \n",
      "Epoch 44/100\n",
      "105/105 [==============================] - 0s - loss: 0.3218 - acc: 0.9810     \n",
      "Epoch 45/100\n",
      "105/105 [==============================] - 0s - loss: 0.3119 - acc: 0.9810     \n",
      "Epoch 46/100\n",
      "105/105 [==============================] - 0s - loss: 0.3054 - acc: 0.9714     \n",
      "Epoch 47/100\n",
      "105/105 [==============================] - 0s - loss: 0.2989 - acc: 0.9810     \n",
      "Epoch 48/100\n",
      "105/105 [==============================] - 0s - loss: 0.2913 - acc: 0.9619     \n",
      "Epoch 49/100\n",
      "105/105 [==============================] - 0s - loss: 0.2909 - acc: 0.9524     \n",
      "Epoch 50/100\n",
      "105/105 [==============================] - 0s - loss: 0.2847 - acc: 0.9619     \n",
      "Epoch 51/100\n",
      "105/105 [==============================] - 0s - loss: 0.2758 - acc: 0.9810     \n",
      "Epoch 52/100\n",
      "105/105 [==============================] - 0s - loss: 0.2696 - acc: 0.9810     \n",
      "Epoch 53/100\n",
      "105/105 [==============================] - 0s - loss: 0.2659 - acc: 0.9810     \n",
      "Epoch 54/100\n",
      "105/105 [==============================] - 0s - loss: 0.2608 - acc: 0.9714     \n",
      "Epoch 55/100\n",
      "105/105 [==============================] - 0s - loss: 0.2553 - acc: 0.9810     \n",
      "Epoch 56/100\n",
      "105/105 [==============================] - 0s - loss: 0.2504 - acc: 0.9810     \n",
      "Epoch 57/100\n",
      "105/105 [==============================] - 0s - loss: 0.2475 - acc: 0.9714     \n",
      "Epoch 58/100\n",
      "105/105 [==============================] - 0s - loss: 0.2407 - acc: 0.9810     \n",
      "Epoch 59/100\n",
      "105/105 [==============================] - 0s - loss: 0.2373 - acc: 0.9905     \n",
      "Epoch 60/100\n",
      "105/105 [==============================] - 0s - loss: 0.2301 - acc: 0.9810     \n",
      "Epoch 61/100\n",
      "105/105 [==============================] - 0s - loss: 0.2281 - acc: 0.9714     \n",
      "Epoch 62/100\n",
      "105/105 [==============================] - 0s - loss: 0.2223 - acc: 0.9714     \n",
      "Epoch 63/100\n",
      "105/105 [==============================] - 0s - loss: 0.2204 - acc: 0.9905     \n",
      "Epoch 64/100\n",
      "105/105 [==============================] - 0s - loss: 0.2157 - acc: 0.9714       \n",
      "Epoch 65/100\n",
      "105/105 [==============================] - 0s - loss: 0.2141 - acc: 0.9810     \n",
      "Epoch 66/100\n",
      "105/105 [==============================] - 0s - loss: 0.2073 - acc: 0.9714     \n",
      "Epoch 67/100\n",
      "105/105 [==============================] - 0s - loss: 0.2044 - acc: 0.9619     \n",
      "Epoch 68/100\n",
      "105/105 [==============================] - 0s - loss: 0.2030 - acc: 0.9810     \n",
      "Epoch 69/100\n",
      "105/105 [==============================] - 0s - loss: 0.1991 - acc: 0.9714     \n",
      "Epoch 70/100\n",
      "105/105 [==============================] - 0s - loss: 0.1933 - acc: 0.9714     \n",
      "Epoch 71/100\n",
      "105/105 [==============================] - 0s - loss: 0.1921 - acc: 0.9714       \n",
      "Epoch 72/100\n",
      "105/105 [==============================] - 0s - loss: 0.1876 - acc: 0.9714     \n",
      "Epoch 73/100\n",
      "105/105 [==============================] - 0s - loss: 0.1835 - acc: 0.9714     \n",
      "Epoch 74/100\n",
      "105/105 [==============================] - 0s - loss: 0.1818 - acc: 0.9714     \n",
      "Epoch 75/100\n",
      "105/105 [==============================] - 0s - loss: 0.1792 - acc: 0.9810     \n",
      "Epoch 76/100\n",
      "105/105 [==============================] - 0s - loss: 0.1769 - acc: 0.9714     \n",
      "Epoch 77/100\n",
      "105/105 [==============================] - 0s - loss: 0.1748 - acc: 0.9714     \n",
      "Epoch 78/100\n",
      "105/105 [==============================] - 0s - loss: 0.1740 - acc: 0.9619     \n",
      "Epoch 79/100\n",
      "105/105 [==============================] - 0s - loss: 0.1688 - acc: 0.9714     \n",
      "Epoch 80/100\n",
      "105/105 [==============================] - 0s - loss: 0.1703 - acc: 0.9714     \n",
      "Epoch 81/100\n",
      "105/105 [==============================] - 0s - loss: 0.1624 - acc: 0.9714     \n",
      "Epoch 82/100\n",
      "105/105 [==============================] - 0s - loss: 0.1641 - acc: 0.9810     \n",
      "Epoch 83/100\n",
      "105/105 [==============================] - 0s - loss: 0.1597 - acc: 0.9714     \n",
      "Epoch 84/100\n",
      "105/105 [==============================] - 0s - loss: 0.1597 - acc: 0.9619     \n",
      "Epoch 85/100\n",
      "105/105 [==============================] - 0s - loss: 0.1558 - acc: 0.9714     \n",
      "Epoch 86/100\n",
      "105/105 [==============================] - 0s - loss: 0.1537 - acc: 0.9714     \n",
      "Epoch 87/100\n",
      "105/105 [==============================] - 0s - loss: 0.1507 - acc: 0.9810     \n",
      "Epoch 88/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - 0s - loss: 0.1495 - acc: 0.9714     \n",
      "Epoch 89/100\n",
      "105/105 [==============================] - 0s - loss: 0.1503 - acc: 0.9714     \n",
      "Epoch 90/100\n",
      "105/105 [==============================] - 0s - loss: 0.1460 - acc: 0.9810     \n",
      "Epoch 91/100\n",
      "105/105 [==============================] - 0s - loss: 0.1444 - acc: 0.9714     \n",
      "Epoch 92/100\n",
      "105/105 [==============================] - 0s - loss: 0.1425 - acc: 0.9810     \n",
      "Epoch 93/100\n",
      "105/105 [==============================] - 0s - loss: 0.1407 - acc: 0.9714     \n",
      "Epoch 94/100\n",
      "105/105 [==============================] - 0s - loss: 0.1383 - acc: 0.9810     \n",
      "Epoch 95/100\n",
      "105/105 [==============================] - 0s - loss: 0.1410 - acc: 0.9714     \n",
      "Epoch 96/100\n",
      "105/105 [==============================] - 0s - loss: 0.1389 - acc: 0.9714     \n",
      "Epoch 97/100\n",
      "105/105 [==============================] - 0s - loss: 0.1354 - acc: 0.9714     \n",
      "Epoch 98/100\n",
      "105/105 [==============================] - 0s - loss: 0.1335 - acc: 0.9714     \n",
      "Epoch 99/100\n",
      "105/105 [==============================] - 0s - loss: 0.1324 - acc: 0.9714     \n",
      "Epoch 100/100\n",
      "105/105 [==============================] - 0s - loss: 0.1301 - acc: 0.9810     \n"
     ]
    }
   ],
   "source": [
    "model_info = model.fit(train_x, train_y, epochs=100, batch_size=1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use keras evaluate() function which returns loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy = 0.9778\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(test_x, test_y, verbose=0)\n",
    "print(\"Model Accuracy = {:.4f}\".format(accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Networks using Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will see some complex problem where deep learning can be used to improve performance. \n",
    "\n",
    "Object recognition is the process to identify predefined objects in the images. In the following example, we will use Keras to create CNN for object recognition.\n",
    "\n",
    "We will use CIFAR-10 dataset in this task. CIFAR stands for Canadian Institute for Advanced Research (CIFAR). This dataset has 60000 images of different objects.\n",
    "\n",
    "This dataset can easily be loaded using keras library. When we execute the code, it download the images. It may take some time depending on your bandwidth. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "module 'keras.backend' has no attribute 'set_image_dim_ordering'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-4bb88d36ea7e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_image_dim_ordering\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'th'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: module 'keras.backend' has no attribute 'set_image_dim_ordering'"
     ]
    }
   ],
   "source": [
    "# import library\n",
    "from keras.datasets import cifar10\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.constraints import maxnorm\n",
    "from scipy.misc import toimage\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers.convolutional import Conv2D\n",
    "\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Loading the data\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-80c2dbc3ab58>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Lets see some examples from this data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mgs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgridspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGridSpec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwspace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "#Lets see some examples from this data\n",
    "\n",
    "fig = plt.figure()\n",
    "gs = gridspec.GridSpec(4, 4, wspace=0.0)\n",
    "ax = [plt.subplot(gs[i]) for i in range(4*4)]\n",
    "for i in range(16):\n",
    "    ax[i].imshow(toimage(x_train[i]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now convert the class to one hot encoding matrix\n",
    "y_train_onehot = np_utils.to_categorical(y_train)\n",
    "y_test_onehot = np_utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use simple CNN architecture to start. In this example we will use two convolutional layers and a max pooling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 32, 16, 16)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               4194816   \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 4,210,090\n",
      "Trainable params: 4,210,090\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "#Sequential model is selected to get a stack of layers.\n",
    "num_classes = 10\n",
    "model = Sequential()\n",
    "\n",
    "#First convolution layer\n",
    "model.add(Conv2D(32, (3, 3), padding='same',input_shape=(3, 32, 32),  activation='relu'))\n",
    "\n",
    "#Second convolution layer\n",
    "model.add(Conv2D(32, (3, 3),padding='same', activation='relu', ))\n",
    "\n",
    "#Pooling\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "#Flatten the output\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "\n",
    "#Output class\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "# Compile model\n",
    "epochs = 50\n",
    "lrate = 0.05\n",
    "\n",
    "sgd = SGD(lr=lrate, momentum=0.8, decay=lrate/epochs, nesterov=False)\n",
    "\n",
    "#Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "#Print summary of CNN\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/250\n",
      "50000/50000 [==============================] - 504s - loss: 14.5052 - acc: 0.0999 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 2/250\n",
      "50000/50000 [==============================] - 496s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 3/250\n",
      "50000/50000 [==============================] - 473s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 4/250\n",
      "50000/50000 [==============================] - 477s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 5/250\n",
      "50000/50000 [==============================] - 474s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 6/250\n",
      "50000/50000 [==============================] - 495s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 7/250\n",
      "50000/50000 [==============================] - 475s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 8/250\n",
      "50000/50000 [==============================] - 479s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 9/250\n",
      "50000/50000 [==============================] - 476s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 10/250\n",
      "50000/50000 [==============================] - 504s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 11/250\n",
      "50000/50000 [==============================] - 484s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 12/250\n",
      "50000/50000 [==============================] - 476s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 13/250\n",
      "50000/50000 [==============================] - 500s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 14/250\n",
      "50000/50000 [==============================] - 486s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 15/250\n",
      "50000/50000 [==============================] - 496s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 16/250\n",
      "50000/50000 [==============================] - 501s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 17/250\n",
      "50000/50000 [==============================] - 516s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 18/250\n",
      "50000/50000 [==============================] - 494s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 19/250\n",
      "50000/50000 [==============================] - 520s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 20/250\n",
      "50000/50000 [==============================] - 507s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 21/250\n",
      "50000/50000 [==============================] - 571s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 22/250\n",
      "50000/50000 [==============================] - 578s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 23/250\n",
      "50000/50000 [==============================] - 546s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 24/250\n",
      "50000/50000 [==============================] - 607s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 25/250\n",
      "50000/50000 [==============================] - 576s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 26/250\n",
      "50000/50000 [==============================] - 514s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 27/250\n",
      "50000/50000 [==============================] - 549s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 28/250\n",
      "50000/50000 [==============================] - 530s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 29/250\n",
      "50000/50000 [==============================] - 557s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 30/250\n",
      "50000/50000 [==============================] - 587s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 31/250\n",
      "50000/50000 [==============================] - 593s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 32/250\n",
      "50000/50000 [==============================] - 588s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 33/250\n",
      "50000/50000 [==============================] - 591s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 34/250\n",
      "50000/50000 [==============================] - 580s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 35/250\n",
      "50000/50000 [==============================] - 1002s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 36/250\n",
      "50000/50000 [==============================] - 1080s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 37/250\n",
      "50000/50000 [==============================] - 1194s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 38/250\n",
      "50000/50000 [==============================] - 1139s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 39/250\n",
      "50000/50000 [==============================] - 537s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 40/250\n",
      "50000/50000 [==============================] - 510s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 41/250\n",
      "50000/50000 [==============================] - 528s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 42/250\n",
      "50000/50000 [==============================] - 510s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 43/250\n",
      "50000/50000 [==============================] - 510s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 44/250\n",
      "50000/50000 [==============================] - 525s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 45/250\n",
      "50000/50000 [==============================] - 508s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 46/250\n",
      "50000/50000 [==============================] - 508s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 47/250\n",
      "50000/50000 [==============================] - 509s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 48/250\n",
      "50000/50000 [==============================] - 525s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 49/250\n",
      "50000/50000 [==============================] - 508s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 50/250\n",
      "50000/50000 [==============================] - 513s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 51/250\n",
      "50000/50000 [==============================] - 528s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 52/250\n",
      "50000/50000 [==============================] - 510s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 53/250\n",
      "50000/50000 [==============================] - 509s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 54/250\n",
      "50000/50000 [==============================] - 511s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 55/250\n",
      "50000/50000 [==============================] - 526s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 56/250\n",
      "50000/50000 [==============================] - 503s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 57/250\n",
      "50000/50000 [==============================] - 506s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 58/250\n",
      "50000/50000 [==============================] - 523s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 59/250\n",
      "50000/50000 [==============================] - 504s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 60/250\n",
      "50000/50000 [==============================] - 506s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 61/250\n",
      "50000/50000 [==============================] - 506s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 62/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 524s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 63/250\n",
      "50000/50000 [==============================] - 506s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 64/250\n",
      "50000/50000 [==============================] - 507s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 65/250\n",
      "50000/50000 [==============================] - 525s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 66/250\n",
      "50000/50000 [==============================] - 506s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 67/250\n",
      "50000/50000 [==============================] - 508s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 68/250\n",
      "50000/50000 [==============================] - 509s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 69/250\n",
      "50000/50000 [==============================] - 527s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 70/250\n",
      "50000/50000 [==============================] - 509s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 71/250\n",
      "50000/50000 [==============================] - 508s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 72/250\n",
      "50000/50000 [==============================] - 528s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 73/250\n",
      "50000/50000 [==============================] - 514s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 74/250\n",
      "50000/50000 [==============================] - 516s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 75/250\n",
      "50000/50000 [==============================] - 520s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 76/250\n",
      "50000/50000 [==============================] - 538s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 77/250\n",
      "50000/50000 [==============================] - 519s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 78/250\n",
      "50000/50000 [==============================] - 544s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 79/250\n",
      "50000/50000 [==============================] - 543s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 80/250\n",
      "50000/50000 [==============================] - 522s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 81/250\n",
      "50000/50000 [==============================] - 524s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 82/250\n",
      "50000/50000 [==============================] - 537s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 83/250\n",
      "50000/50000 [==============================] - 532s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 84/250\n",
      "50000/50000 [==============================] - 527s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 85/250\n",
      "50000/50000 [==============================] - 527s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 86/250\n",
      "50000/50000 [==============================] - 545s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 87/250\n",
      "50000/50000 [==============================] - 526s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 88/250\n",
      "50000/50000 [==============================] - 528s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 89/250\n",
      "50000/50000 [==============================] - 548s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 90/250\n",
      "50000/50000 [==============================] - 529s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 91/250\n",
      "50000/50000 [==============================] - 532s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 92/250\n",
      "50000/50000 [==============================] - 537s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 93/250\n",
      "50000/50000 [==============================] - 543s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 94/250\n",
      "50000/50000 [==============================] - 532s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 95/250\n",
      "50000/50000 [==============================] - 531s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 96/250\n",
      "50000/50000 [==============================] - 551s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 97/250\n",
      "50000/50000 [==============================] - 530s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 98/250\n",
      "50000/50000 [==============================] - 534s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 99/250\n",
      "50000/50000 [==============================] - 550s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 100/250\n",
      "50000/50000 [==============================] - 531s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 101/250\n",
      "50000/50000 [==============================] - 534s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 102/250\n",
      "50000/50000 [==============================] - 543s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 103/250\n",
      "50000/50000 [==============================] - 551s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 104/250\n",
      "50000/50000 [==============================] - 534s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 105/250\n",
      "50000/50000 [==============================] - 538s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 106/250\n",
      "50000/50000 [==============================] - 556s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 107/250\n",
      "50000/50000 [==============================] - 508s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 108/250\n",
      "50000/50000 [==============================] - 544s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 109/250\n",
      "50000/50000 [==============================] - 583s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 110/250\n",
      "50000/50000 [==============================] - 545s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 111/250\n",
      "50000/50000 [==============================] - 699s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 112/250\n",
      "50000/50000 [==============================] - 804s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 113/250\n",
      "50000/50000 [==============================] - 662s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 114/250\n",
      "50000/50000 [==============================] - 616s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 115/250\n",
      "50000/50000 [==============================] - 660s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 116/250\n",
      "50000/50000 [==============================] - 696s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 117/250\n",
      "50000/50000 [==============================] - 637s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 118/250\n",
      "50000/50000 [==============================] - 556s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 119/250\n",
      "50000/50000 [==============================] - 558s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 120/250\n",
      "50000/50000 [==============================] - 563s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 121/250\n",
      "50000/50000 [==============================] - 579s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 122/250\n",
      "50000/50000 [==============================] - 566s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 123/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 568s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 124/250\n",
      "50000/50000 [==============================] - 686s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 125/250\n",
      "50000/50000 [==============================] - 636s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 126/250\n",
      "50000/50000 [==============================] - 750s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 127/250\n",
      "50000/50000 [==============================] - 505s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 128/250\n",
      "50000/50000 [==============================] - 560s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 129/250\n",
      "50000/50000 [==============================] - 568s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 130/250\n",
      "50000/50000 [==============================] - 582s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 131/250\n",
      "50000/50000 [==============================] - 562s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 132/250\n",
      "50000/50000 [==============================] - 559s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 133/250\n",
      "50000/50000 [==============================] - 598s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 134/250\n",
      "50000/50000 [==============================] - 584s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 135/250\n",
      "50000/50000 [==============================] - 578s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 136/250\n",
      "50000/50000 [==============================] - 620s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 137/250\n",
      "50000/50000 [==============================] - 624s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 138/250\n",
      "50000/50000 [==============================] - 622s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 139/250\n",
      "50000/50000 [==============================] - 652s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 140/250\n",
      "50000/50000 [==============================] - 625s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 141/250\n",
      "50000/50000 [==============================] - 593s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 142/250\n",
      "50000/50000 [==============================] - 595s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 143/250\n",
      "50000/50000 [==============================] - 573s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 144/250\n",
      "50000/50000 [==============================] - 575s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 145/250\n",
      "50000/50000 [==============================] - 593s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 146/250\n",
      "50000/50000 [==============================] - 573s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 147/250\n",
      "50000/50000 [==============================] - 600s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 148/250\n",
      "50000/50000 [==============================] - 693s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 149/250\n",
      "50000/50000 [==============================] - 660s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 150/250\n",
      "50000/50000 [==============================] - 676s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 151/250\n",
      "50000/50000 [==============================] - 678s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 152/250\n",
      "50000/50000 [==============================] - 664s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 153/250\n",
      "50000/50000 [==============================] - 652s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 154/250\n",
      "50000/50000 [==============================] - 579s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 155/250\n",
      "50000/50000 [==============================] - 556s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 156/250\n",
      "50000/50000 [==============================] - 579s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 157/250\n",
      "50000/50000 [==============================] - 554s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 158/250\n",
      "50000/50000 [==============================] - 565s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 159/250\n",
      "50000/50000 [==============================] - 582s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 160/250\n",
      "50000/50000 [==============================] - 507s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 161/250\n",
      "50000/50000 [==============================] - 544s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 162/250\n",
      "50000/50000 [==============================] - 539s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 163/250\n",
      "50000/50000 [==============================] - 604s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 164/250\n",
      "50000/50000 [==============================] - 545s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 165/250\n",
      "50000/50000 [==============================] - 544s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 166/250\n",
      "50000/50000 [==============================] - 553s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 167/250\n",
      "50000/50000 [==============================] - 540s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 168/250\n",
      "50000/50000 [==============================] - 554s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 169/250\n",
      "50000/50000 [==============================] - 567s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 170/250\n",
      "50000/50000 [==============================] - 557s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 171/250\n",
      "50000/50000 [==============================] - 558s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 172/250\n",
      "50000/50000 [==============================] - 573s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 173/250\n",
      "50000/50000 [==============================] - 645s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 174/250\n",
      "50000/50000 [==============================] - 621s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 175/250\n",
      "50000/50000 [==============================] - 540s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 176/250\n",
      "50000/50000 [==============================] - 479s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 177/250\n",
      "50000/50000 [==============================] - 499s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 178/250\n",
      "50000/50000 [==============================] - 508s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 179/250\n",
      "50000/50000 [==============================] - 490s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 180/250\n",
      "50000/50000 [==============================] - 488s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 181/250\n",
      "50000/50000 [==============================] - 517s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 182/250\n",
      "50000/50000 [==============================] - 482s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 183/250\n",
      "50000/50000 [==============================] - 483s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 184/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 505s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 185/250\n",
      "50000/50000 [==============================] - 487s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 186/250\n",
      "50000/50000 [==============================] - 483s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 187/250\n",
      "50000/50000 [==============================] - 482s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 188/250\n",
      "50000/50000 [==============================] - 506s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 189/250\n",
      "50000/50000 [==============================] - 478s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 190/250\n",
      "50000/50000 [==============================] - 481s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 191/250\n",
      "50000/50000 [==============================] - 480s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 192/250\n",
      "50000/50000 [==============================] - 506s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 193/250\n",
      "50000/50000 [==============================] - 480s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 194/250\n",
      "50000/50000 [==============================] - 483s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 195/250\n",
      "50000/50000 [==============================] - 500s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 196/250\n",
      "50000/50000 [==============================] - 486s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 197/250\n",
      "50000/50000 [==============================] - 480s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 198/250\n",
      "50000/50000 [==============================] - 485s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 199/250\n",
      "50000/50000 [==============================] - 508s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 200/250\n",
      "50000/50000 [==============================] - 492s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 201/250\n",
      "50000/50000 [==============================] - 485s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 202/250\n",
      "50000/50000 [==============================] - 483s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 203/250\n",
      "50000/50000 [==============================] - 502s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 204/250\n",
      "50000/50000 [==============================] - 480s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 205/250\n",
      "50000/50000 [==============================] - 488s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 206/250\n",
      "50000/50000 [==============================] - 504s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 207/250\n",
      "50000/50000 [==============================] - 485s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 208/250\n",
      "50000/50000 [==============================] - 478s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 209/250\n",
      "50000/50000 [==============================] - 479s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 210/250\n",
      "50000/50000 [==============================] - 508s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 211/250\n",
      "50000/50000 [==============================] - 483s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 212/250\n",
      "50000/50000 [==============================] - 487s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 213/250\n",
      "50000/50000 [==============================] - 475s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 214/250\n",
      "50000/50000 [==============================] - 497s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 215/250\n",
      "50000/50000 [==============================] - 471s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 216/250\n",
      "50000/50000 [==============================] - 469s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 217/250\n",
      "50000/50000 [==============================] - 480s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 218/250\n",
      "50000/50000 [==============================] - 485s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 219/250\n",
      "50000/50000 [==============================] - 471s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 220/250\n",
      "50000/50000 [==============================] - 473s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 221/250\n",
      "50000/50000 [==============================] - 497s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 222/250\n",
      "50000/50000 [==============================] - 473s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 223/250\n",
      "50000/50000 [==============================] - 473s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 224/250\n",
      "50000/50000 [==============================] - 472s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 225/250\n",
      "50000/50000 [==============================] - 497s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 226/250\n",
      "50000/50000 [==============================] - 472s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 227/250\n",
      "50000/50000 [==============================] - 471s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 228/250\n",
      "50000/50000 [==============================] - 472s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 229/250\n",
      "50000/50000 [==============================] - 496s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 230/250\n",
      "50000/50000 [==============================] - 474s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 231/250\n",
      "50000/50000 [==============================] - 472s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 232/250\n",
      "50000/50000 [==============================] - 481s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 233/250\n",
      "50000/50000 [==============================] - 490s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 234/250\n",
      "50000/50000 [==============================] - 474s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 235/250\n",
      "50000/50000 [==============================] - 475s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 236/250\n",
      "50000/50000 [==============================] - 500s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 237/250\n",
      "50000/50000 [==============================] - 477s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 238/250\n",
      "50000/50000 [==============================] - 475s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 239/250\n",
      "50000/50000 [==============================] - 474s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 240/250\n",
      "50000/50000 [==============================] - 502s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 241/250\n",
      "50000/50000 [==============================] - 475s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 242/250\n",
      "50000/50000 [==============================] - 473s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 243/250\n",
      "50000/50000 [==============================] - 474s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 244/250\n",
      "50000/50000 [==============================] - 497s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 245/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 476s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 246/250\n",
      "50000/50000 [==============================] - 729s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 247/250\n",
      "50000/50000 [==============================] - 838s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 248/250\n",
      "50000/50000 [==============================] - 811s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 249/250\n",
      "50000/50000 [==============================] - 839s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 250/250\n",
      "50000/50000 [==============================] - 806s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x26293410b38>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fitting the model\n",
    "model.fit(x_train, y_train_onehot, validation_data=(x_test, y_test_onehot), epochs=250, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy = 0.1000\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation of the model\n",
    "loss,accuracy = model.evaluate(x_test, y_test_onehot, verbose=0)\n",
    "print(\"Model Accuracy = {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}