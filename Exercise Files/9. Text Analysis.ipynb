{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Text Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Declare text\n",
    "text1 = \"The Vikram Sarabhai Space Centre is a space research Centre of the ISRO, focused on rocket and space vehicles\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Length of the string - count of characters\n",
    "len(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Length of words\n",
    "words1 = text1.split(\" \")\n",
    "len(words1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Vikram'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1[4]\n",
    "text1[4:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'Vikram', 'Sarabhai', 'Space', 'Centre', 'Centre']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get all words which has first character Capital\n",
    "[wordCap for wordCap in words1 if wordCap.istitle()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Vikram',\n",
       " 'Sarabhai',\n",
       " 'Centre',\n",
       " 'research',\n",
       " 'Centre',\n",
       " 'focused',\n",
       " 'rocket',\n",
       " 'vehicles']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Words with greater than length 5\n",
    "[wordG5 for wordG5 in words1 if len(wordG5)>5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The vikram sarabhai space centre is a space research centre of the isro, focused on rocket and space vehicles'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1.capitalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Vikram Sarabhai Space Centre Is A Space Research Centre Of The Isro, Focused On Rocket And Space Vehicles'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1.title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the vikram sarabhai space centre is a space research centre of the isro, focused on rocket and space vehicles'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'THE VIKRAM SARABHAI SPACE CENTRE IS A SPACE RESEARCH CENTRE OF THE ISRO, FOCUSED ON ROCKET AND SPACE VEHICLES'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tHE vIKRAM sARABHAI sPACE cENTRE IS A SPACE RESEARCH cENTRE OF THE isro, FOCUSED ON ROCKET AND SPACE VEHICLES'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1.swapcase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the vikram sarabhai space centre is a space research centre of the isro, focused on rocket and space vehicles'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1.casefold()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1.index('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1.find('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1.rfind('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The Vikram Sarabhai Space Centre is a space research Centre of the ISRO, focused on rocket and space vehicles']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1.splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Vikram Sarabhai Space Centre is a space research Centre of the ISRO, focused on rocket and space vehicles'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1.lstrip(\"The\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pattern exists  month:Jan\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text2 = \"This news article is published on month:Jan\"\n",
    "matchResult = re.search(r'month:\\w\\w\\w',text2)\n",
    "if matchResult:\n",
    "    print('Pattern exists ', matchResult.group())\n",
    "else:\n",
    "    print('Pattern not exists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_sre.SRE_Match object; span=(34, 43), match='month:Jan'>\n"
     ]
    }
   ],
   "source": [
    "print(matchResult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pattern exists  ab\n"
     ]
    }
   ],
   "source": [
    "matchResult = re.search(r'de', 'abcdef') #Found, matchResult.group() - de\n",
    "matchResult = re.search(r'..cd', 'abcdef') #Found, matchResult.group() - abcd\n",
    "matchResult = re.search(r'\\w\\w\\w', '%%abc') #Found, matchResult.group() - abc\n",
    "matchResult = re.search(r'ef$','abcdef')  #Found, matchResult.group() - ef\n",
    "matchResult = re.search(r'^ab','abcdef')  #Found, matchResult.group() - ab\n",
    "\n",
    "if matchResult:\n",
    "    print('Pattern exists ', matchResult.group())\n",
    "else:\n",
    "    print('Pattern not exists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pattern exists  ef\n"
     ]
    }
   ],
   "source": [
    "matchResult = re.search(r'de+f', 'abcdeef') #Found, matchResult.group() - deef\n",
    "matchResult = re.search(r'dk*e', 'abcdeef') #Found, matchResult.group() - de\n",
    "matchResult = re.search(r'e?f', 'abcdef') #Found, matchResult.group() - ef\n",
    "\n",
    "\n",
    "if matchResult:\n",
    "    print('Pattern exists ', matchResult.group())\n",
    "else:\n",
    "    print('Pattern not exists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pattern exists  #abc\n"
     ]
    }
   ],
   "source": [
    "matchResult = re.search(r'#[abc]+', '#abcdef') #Found, matchResult.group() - #abc\n",
    "\n",
    "\n",
    "if matchResult:\n",
    "    print('Pattern exists ', matchResult.group())\n",
    "else:\n",
    "    print('Pattern not exists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#datascience\n",
      "#python\n",
      "#machinelearning\n"
     ]
    }
   ],
   "source": [
    "## Consider a tweet with hashtags\n",
    "tweet = 'I am learning #datascience and it is awesome. #python #machinelearning'\n",
    "\n",
    "## Here re.findall() returns a list of all hashtags present in the tweet.\n",
    "hastags = re.findall(r'#\\w+', tweet)\n",
    "for tag in hastags:\n",
    "    print(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Text: Moby Dick by Herman Melville 1851>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sent1: Call me Ishmael .\n",
      "sent2: The family of Dashwood had long been settled in Sussex .\n",
      "sent3: In the beginning God created the heaven and the earth .\n",
      "sent4: Fellow - Citizens of the Senate and of the House of Representatives :\n",
      "sent5: I have a problem with people PMing me to lol JOIN\n",
      "sent6: SCENE 1 : [ wind ] [ clop clop clop ] KING ARTHUR : Whoa there !\n",
      "sent7: Pierre Vinken , 61 years old , will join the board as a nonexecutive director Nov. 29 .\n",
      "sent8: 25 SEXY MALE , seeks attrac older single lady , for discreet encounters .\n",
      "sent9: THE suburb of Saffron Park lay on the sunset side of London , as red and ragged as a cloud of sunset .\n"
     ]
    }
   ],
   "source": [
    "#Show one sentence each from all the above corpora\n",
    "sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Call', 'me', 'Ishmael', '.']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check any sentence\n",
    "sent1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19317\n"
     ]
    }
   ],
   "source": [
    "print(len(set(text1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "freqCount = FreqDist(text3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabulary = freqCount.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqCount['The']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fish', 'fish', 'fish']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fishwords = ['fishes','Fishings','Fishes']\n",
    "prt = nltk.PorterStemmer()\n",
    "[prt.stem(ts) for ts in fishwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fish', 'Fishings', 'Fishes']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fishwords = ['fishes','Fishings','Fishes']\n",
    "WNlemma = nltk.WordNetLemmatizer()\n",
    "[WNlemma.lemmatize(ts) for ts in fishwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Why', 'are', 'you', 'so', 'intelligent?']\n",
      "['Why', 'are', 'you', 'so', 'intelligent', '?']\n"
     ]
    }
   ],
   "source": [
    "text2 = \"Why are you so intelligent?\"\n",
    "words = text2.split(' ')\n",
    "print(words)\n",
    "print(nltk.word_tokenize(text2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['His name John', ' He lives in U', 'K', ' with his wife Is he the best? No he is not!']\n",
      "['His name John.', 'He lives in U.K. with his wife Is he the best?', 'No he is not!']\n"
     ]
    }
   ],
   "source": [
    "text3 = \"His name John. He lives in U.K. with his wife Is he the best? No he is not!\"\n",
    "sent1 = text3.split(\".\")\n",
    "print(sent1)\n",
    "sent2 = nltk.sent_tokenize(text3)\n",
    "print(sent2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Why', 'WRB'),\n",
       " ('are', 'VBP'),\n",
       " ('you', 'PRP'),\n",
       " ('so', 'IN'),\n",
       " ('intelligent', 'JJ'),\n",
       " ('?', '.')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_words = nltk.word_tokenize(text2)\n",
    "nltk.pos_tag(tokenize_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Why', 'WRB'),\n",
       " ('are', 'VBP'),\n",
       " ('you', 'PRP'),\n",
       " ('so', 'IN'),\n",
       " ('intelligent', 'JJ'),\n",
       " ('?', '.')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(tokenize_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (NP Alice) (VP (V loves) (NP Bob)))\n"
     ]
    }
   ],
   "source": [
    "# Parsing sentence structure\n",
    "text15 = nltk.word_tokenize(\"Alice loves Bob\")\n",
    "grammar = nltk.CFG.fromstring(\"\"\"\n",
    "S -> NP VP\n",
    "VP -> V NP\n",
    "NP -> 'Alice' | 'Bob'\n",
    "V -> 'loves'\n",
    "\"\"\")\n",
    "\n",
    "parser = nltk.ChartParser(grammar)\n",
    "trees = parser.parse_all(text15)\n",
    "for tree in trees:\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import datasets\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 20news dataset. This may take a few minutes.\n",
      "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
     ]
    }
   ],
   "source": [
    "#This data is well separated in training and testing. We can use training data for training our model and test performance on test data\n",
    "training_data = fetch_20newsgroups(subset='train', shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['soc.religion.christian',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'sci.crypt',\n",
       " 'comp.windows.x',\n",
       " 'rec.sport.baseball',\n",
       " 'sci.med',\n",
       " 'comp.graphics',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'talk.politics.misc',\n",
       " 'sci.space',\n",
       " 'rec.sport.hockey',\n",
       " 'talk.religion.misc',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'misc.forsale',\n",
       " 'talk.politics.mideast',\n",
       " 'alt.atheism',\n",
       " 'sci.electronics',\n",
       " 'talk.politics.guns']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We can check for labels in the training dataset\n",
    "import random\n",
    "from random import shuffle\n",
    "a =  random.shuffle(training_data.target_names)\n",
    "type(training_data.target_names)\n",
    "import numpy as np    \n",
    "np.random.shuffle(training_data.target_names)\n",
    "training_data.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: lerxst@wam.umd.edu (where's my thing)\n",
      "Subject: WHAT car is this!?\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\n",
      "Organization: University of Maryland, College Park\n",
      "Lines: 15\n",
      "\n",
      " I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      "\n",
      "Thanks,\n",
      "- IL\n",
      "   ---- brought to you by your neighborhood Lerxst ----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(training_data.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Convert these sentences to word vector. We will do this by CountVectorizer available in scikit-learn\n",
    "#First we need to import it\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now we need to create its object and fit the model. \n",
    "count_vector = CountVectorizer()\n",
    "count_vector.fit(training_data.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#We need to transform the data using fitted model above\n",
    "training_count = count_vector.transform(training_data.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11314, 130107)\n"
     ]
    }
   ],
   "source": [
    "#We can check shape of our matrix\n",
    "print(training_count.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#In above matrix we are storing only count of words. We can weight them based on importance. We can use TF-IDF to do that.\n",
    "#Import TFIDF\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create Object and fit\n",
    "tfidf_model = TfidfTransformer()\n",
    "tfidf_model.fit(training_count)\n",
    "training_tfidf = tfidf_model.transform(training_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Fit model on training data\n",
    "NBModel = MultinomialNB().fit(training_tfidf, training_data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Evaluate Model\n",
    "testing_data = fetch_20newsgroups(subset='test', shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Convert Testing data in the correct format\n",
    "testing_count = count_vector.transform(testing_data.data)\n",
    "testing_tfidf = tfidf_model.transform(testing_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Predict newsgroups\n",
    "predicted = NBModel.predict(testing_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.77389803505\n"
     ]
    }
   ],
   "source": [
    "#Get Accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(testing_data.target,predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#We can use nlp techniques to improve the result\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create a set of stop words\n",
    "from nltk.corpus import stopwords \n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def data_cleaner(raw_data):\n",
    "    \n",
    "    cleaned_data = list()\n",
    "    for row in raw_data:\n",
    "        stemmed_words = list()\n",
    "    \n",
    "        #Find meaningful tokesn\n",
    "        tokens = nltk.word_tokenize(row)\n",
    "    \n",
    "        #Convert words using stemming\n",
    "        prt = nltk.PorterStemmer()\n",
    "    \n",
    "        for token in tokens:\n",
    "            if token not in stop_words:\n",
    "                if(token.isalnum()==True):\n",
    "                    stemmed_words.append(prt.stem(token) )\n",
    "    \n",
    "        sent = ' '.join(stemmed_words)\n",
    "        cleaned_data.append(sent)\n",
    "    return cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_data_modified = data_cleaner(training_data.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from lerxst thing subject what car organ univers maryland colleg park line 15 I wonder anyon could enlighten car I saw day It sport car look late earli 70 It call bricklin the door realli small In addit front bumper separ rest bodi thi I know If anyon tellm model name engin spec year product car made histori whatev info funki look car pleas thank IL brought neighborhood lerxst\n"
     ]
    }
   ],
   "source": [
    "print(training_data_modified[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from guykuo guy kuo subject SI clock poll final call summari final call SI clock report keyword SI acceler clock upgrad organ univers washington line 11 A fair number brave soul upgrad SI clock oscil share experi poll pleas send brief messag detail experi procedur top speed attain cpu rate speed add card adapt heat sink hour usag per day floppi disk function 800 floppi especi request I summar next two day pleas add network knowledg base done clock upgrad answer poll thank guy kuo guykuo'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_modified[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data_modified = data_cleaner(testing_data.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Transform and do training.\n",
    "count_vector.fit(training_data_modified)\n",
    "training_count = count_vector.transform(training_data_modified)\n",
    "tfidf_model.fit(training_count)\n",
    "training_tfidf = tfidf_model.transform(training_count)\n",
    "NBModel = MultinomialNB().fit(training_tfidf, training_data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testing_count = count_vector.transform(test_data_modified)\n",
    "testing_tfidf = tfidf_model.transform(testing_count)\n",
    "predicted = NBModel.predict(testing_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.802841210834\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(testing_data.target,predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from jgreen amber joe green subject Re weitek p9000 organ harri comput system divis line 14 distribut world tin version pl9 robert kyanko rob wrote abraxi write articl anyon know weitek p9000 graphic chip As far stuff goe look pretti nice It got quadrilater fill command requir four point Do weitek number I like get inform chip joe green harri corpor jgreen comput system divis the thing realli scare person sens humor jonathan winter'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_modified[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Take data. We will use sample of 1000 rows from 20newsgroup data for this task\n",
    "import random\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "training_data = fetch_20newsgroups(subset='train', shuffle=True)\n",
    "data500 = random.sample(training_data.data, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Clean the data\n",
    "from nltk.corpus import stopwords \n",
    "stop_words = set(stopwords.words('english'))\n",
    "training_data_modified = list()\n",
    "for training_example in data500:\n",
    "    stemmed_words = list()\n",
    "    \n",
    "    #Find meaningful tokesn\n",
    "    tokens = nltk.word_tokenize(training_example)\n",
    "    \n",
    "    #Convert words using stemming\n",
    "    prt = nltk.PorterStemmer()\n",
    "    \n",
    "    for token in tokens:\n",
    "        if token not in stop_words:\n",
    "            if(token.isalnum()==True):\n",
    "                stemmed_words.append(prt.stem(token) )\n",
    "    \n",
    "    sent = ' '.join(stemmed_words)\n",
    "    training_data_modified.append(sent)\n",
    "training_data_cleaned = [doc.split() for doc in training_data_modified]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Software\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "#Gensim is used to handle text data and convert corpus into document term matrix\n",
    "import gensim\n",
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create document term matrix\n",
    "dictionary = corpora.Dictionary(training_data_cleaned)\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in training_data_cleaned]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create model using gensim\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "# Training LDA Model\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=3, id2word = dictionary, passes=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.013*\"the\" + 0.010*\"I\" + 0.006*\"A\"'), (1, '0.030*\"I\" + 0.008*\"one\" + 0.007*\"the\"'), (2, '0.023*\"I\" + 0.009*\"the\" + 0.008*\"line\"')]\n"
     ]
    }
   ],
   "source": [
    "#Each line represent topic with words. \n",
    "print(ldamodel.print_topics(num_topics=3, num_words=3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
